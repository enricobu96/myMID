{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "from environment import Environment, Scene, Node, derivative_of\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "desired_max_time = 100\n",
    "pred_indices = [2, 3]\n",
    "state_dim = 6\n",
    "frame_diff = 10\n",
    "desired_frame_diff = 1\n",
    "dt = 0.4\n",
    "\n",
    "standardization = {\n",
    "    'PEDESTRIAN': {\n",
    "        'position': {\n",
    "            'x': {'mean': 0, 'std': 1},\n",
    "            'y': {'mean': 0, 'std': 1}\n",
    "        },\n",
    "        'velocity': {\n",
    "            'x': {'mean': 0, 'std': 2},\n",
    "            'y': {'mean': 0, 'std': 2}\n",
    "        },\n",
    "        'acceleration': {\n",
    "            'x': {'mean': 0, 'std': 1},\n",
    "            'y': {'mean': 0, 'std': 1}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def maybe_makedirs(path_to_create):\n",
    "    \"\"\"\n",
    "    This function will create a directory, unless it exists already,\n",
    "    at which point the function will return.\n",
    "    The exception handling is necessary as it prevents a race condition\n",
    "    from occurring.\n",
    "    Inputs:\n",
    "        path_to_create - A string path to a directory you'd like created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(path_to_create)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path_to_create):\n",
    "            raise\n",
    "\n",
    "def augment_scene(scene, angle):\n",
    "    \"\"\"\n",
    "    Data augmentation function for scene (rotation). Returns Scene object with augmented data.\n",
    "    \"\"\"\n",
    "    def rotate_pc(pc, alpha):\n",
    "        M = np.array([[np.cos(alpha), -np.sin(alpha)],\n",
    "                      [np.sin(alpha), np.cos(alpha)]])\n",
    "        return M @ pc\n",
    "\n",
    "    data_columns = pd.MultiIndex.from_product([['position', 'velocity', 'acceleration'], ['x', 'y']])\n",
    "\n",
    "    scene_aug = Scene(timesteps=scene.timesteps, dt=scene.dt, name=scene.name)\n",
    "\n",
    "    alpha = angle * np.pi / 180\n",
    "\n",
    "    for node in scene.nodes:\n",
    "        x = node.data.position.x.copy()\n",
    "        y = node.data.position.y.copy()\n",
    "\n",
    "        x, y = rotate_pc(np.array([x, y]), alpha)\n",
    "\n",
    "        vx = derivative_of(x, scene.dt)\n",
    "        vy = derivative_of(y, scene.dt)\n",
    "        ax = derivative_of(vx, scene.dt)\n",
    "        ay = derivative_of(vy, scene.dt)\n",
    "\n",
    "        data_dict = {('position', 'x'): x,\n",
    "                     ('position', 'y'): y,\n",
    "                     ('velocity', 'x'): vx,\n",
    "                     ('velocity', 'y'): vy,\n",
    "                     ('acceleration', 'x'): ax,\n",
    "                     ('acceleration', 'y'): ay}\n",
    "\n",
    "        node_data = pd.DataFrame(data_dict, columns=data_columns)\n",
    "\n",
    "        # Create node and add it to the scene\n",
    "        node = Node(node_type=node.type, node_id=node.id, data=node_data, first_timestep=node.first_timestep)\n",
    "\n",
    "        scene_aug.nodes.append(node)\n",
    "    return scene_aug\n",
    "\n",
    "\n",
    "def augment(scene):\n",
    "    \"\"\"\n",
    "    Executes data augmentation.\n",
    "    \"\"\"\n",
    "    scene_aug = np.random.choice(scene.augmented)\n",
    "    scene_aug.temporal_scene_graph = scene.temporal_scene_graph\n",
    "    return scene_aug\n",
    "\n",
    "def swap_columns(df, col1, col2):\n",
    "    col_list = list(df.columns)\n",
    "    x, y = col_list.index(col1), col_list.index(col2)\n",
    "    col_list[y], col_list[x] = col_list[x], col_list[y]\n",
    "    df = df[col_list]\n",
    "    return df\n",
    "\n",
    "def fix_for_assert(df):\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "nl = 0\n",
    "l = 0\n",
    "\n",
    "data_folder_name = 'processed_data_noise'\n",
    "\n",
    "maybe_makedirs(data_folder_name)\n",
    "data_columns = pd.MultiIndex.from_product([['position', 'velocity', 'acceleration'], ['x', 'y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SDD train\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "Scene: Duration: 175.60000000000002s, Nodes: 138, Map: No.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "Scene: Duration: 193.20000000000002s, Nodes: 78, Map: No.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "Scene: Duration: 187.60000000000002s, Nodes: 54, Map: No.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "Scene: Duration: 194.0s, Nodes: 47, Map: No.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/enrico/Repository/myMID/process_data_longterm.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000003?line=88'>89</a>\u001b[0m     angles \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39m360\u001b[39m, \u001b[39m15\u001b[39m) \u001b[39mif\u001b[39;00m data_class \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m [\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000003?line=89'>90</a>\u001b[0m     \u001b[39mfor\u001b[39;00m angle \u001b[39min\u001b[39;00m angles:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000003?line=90'>91</a>\u001b[0m         scene\u001b[39m.\u001b[39maugmented\u001b[39m.\u001b[39mappend(augment_scene(scene, angle))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000003?line=92'>93</a>\u001b[0m \u001b[39mprint\u001b[39m(scene)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000003?line=93'>94</a>\u001b[0m scenes\u001b[39m.\u001b[39mappend(scene)\n",
      "\u001b[1;32m/home/enrico/Repository/myMID/process_data_longterm.ipynb Cell 2'\u001b[0m in \u001b[0;36maugment_scene\u001b[0;34m(scene, angle)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=77'>78</a>\u001b[0m ay \u001b[39m=\u001b[39m derivative_of(vy, scene\u001b[39m.\u001b[39mdt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=79'>80</a>\u001b[0m data_dict \u001b[39m=\u001b[39m {(\u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m): x,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=80'>81</a>\u001b[0m              (\u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m): y,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=81'>82</a>\u001b[0m              (\u001b[39m'\u001b[39m\u001b[39mvelocity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m): vx,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=82'>83</a>\u001b[0m              (\u001b[39m'\u001b[39m\u001b[39mvelocity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m): vy,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=83'>84</a>\u001b[0m              (\u001b[39m'\u001b[39m\u001b[39macceleration\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m): ax,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=84'>85</a>\u001b[0m              (\u001b[39m'\u001b[39m\u001b[39macceleration\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m): ay}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=86'>87</a>\u001b[0m node_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data_dict, columns\u001b[39m=\u001b[39;49mdata_columns)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=88'>89</a>\u001b[0m \u001b[39m# Create node and add it to the scene\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/enrico/Repository/myMID/process_data_longterm.ipynb#ch0000001?line=89'>90</a>\u001b[0m node \u001b[39m=\u001b[39m Node(node_type\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39mtype, node_id\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39mid, data\u001b[39m=\u001b[39mnode_data, first_timestep\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39mfirst_timestep)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:435\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=430'>431</a>\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=431'>432</a>\u001b[0m         data, axes\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns), dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=432'>433</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=433'>434</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=434'>435</a>\u001b[0m     mgr \u001b[39m=\u001b[39m init_dict(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=435'>436</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=436'>437</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:228\u001b[0m, in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=223'>224</a>\u001b[0m missing \u001b[39m=\u001b[39m arrays\u001b[39m.\u001b[39misna()\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=224'>225</a>\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=225'>226</a>\u001b[0m     \u001b[39m# GH10856\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=226'>227</a>\u001b[0m     \u001b[39m# raise ValueError if only scalars in dict\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=227'>228</a>\u001b[0m     index \u001b[39m=\u001b[39m extract_index(arrays[\u001b[39m~\u001b[39;49mmissing])\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=228'>229</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=229'>230</a>\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:380\u001b[0m, in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=377'>378</a>\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=378'>379</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=379'>380</a>\u001b[0m             index \u001b[39m=\u001b[39m ibase\u001b[39m.\u001b[39;49mdefault_index(lengths[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py?line=381'>382</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ensure_index(index)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:5391\u001b[0m, in \u001b[0;36mdefault_index\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m   <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5387'>5388</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_index\u001b[39m(n):\n\u001b[1;32m   <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5388'>5389</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mindexes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrange\u001b[39;00m \u001b[39mimport\u001b[39;00m RangeIndex\n\u001b[0;32m-> <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5390'>5391</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m RangeIndex(\u001b[39m0\u001b[39;49m, n, name\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py:96\u001b[0m, in \u001b[0;36mRangeIndex.__new__\u001b[0;34m(cls, start, stop, step, dtype, copy, name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py?line=92'>93</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_simple_new(start, dtype\u001b[39m=\u001b[39mdtype, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m     <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py?line=94'>95</a>\u001b[0m \u001b[39m# validate the arguments\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py?line=95'>96</a>\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39;49mall_none(start, stop, step):\n\u001b[1;32m     <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py?line=96'>97</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mRangeIndex(...) must be called with integers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py?line=98'>99</a>\u001b[0m start \u001b[39m=\u001b[39m ensure_python_int(start) \u001b[39mif\u001b[39;00m start \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/common.py:187\u001b[0m, in \u001b[0;36mall_none\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/common.py?line=182'>183</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_none\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/common.py?line=183'>184</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/common.py?line=184'>185</a>\u001b[0m \u001b[39m    Returns a boolean indicating if all arguments are None.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/common.py?line=185'>186</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/enrico/.local/lib/python3.8/site-packages/pandas/core/common.py?line=186'>187</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mall\u001b[39;49m(arg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mfor\u001b[39;49;00m arg \u001b[39min\u001b[39;49;00m args)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "colpevoli_train = []\n",
    "colpevoli_test = []\n",
    "\n",
    "for data_class in [\"train\", \"test\"]:\n",
    "    raw_path = \"raw_data/stanford/longterm\"\n",
    "    out_path = \"processed_data_noise\"\n",
    "    data_path = os.path.join(raw_path, f\"{data_class}_longterm.pkl\")\n",
    "    print(f\"Processing SDD {data_class}\")\n",
    "    data_out_path = os.path.join(out_path, f\"sdd_{data_class}.pkl\")\n",
    "    df = pickle.load(open(data_path, \"rb\"))\n",
    "    df = swap_columns(df, 'trackId', 'frame')\n",
    "    env = Environment(node_type_list=['PEDESTRIAN'], standardization=standardization)\n",
    "    attention_radius = dict()\n",
    "    attention_radius[(env.NodeType.PEDESTRIAN, env.NodeType.PEDESTRIAN)] = 3.0\n",
    "    env.attention_radius = attention_radius\n",
    "\n",
    "    scenes = []\n",
    "\n",
    "    group = df.groupby(\"sceneId\")\n",
    "\n",
    "    for scene, data in group:\n",
    "        data['frame'] = pd.to_numeric(data['frame'], downcast='integer')\n",
    "        data['trackId'] = pd.to_numeric(data['trackId'], downcast='integer')\n",
    "\n",
    "        data['frame'] = data['frame'] // 30\n",
    "\n",
    "        data['frame'] -= data['frame'].min()\n",
    "\n",
    "        data['node_type'] = 'PEDESTRIAN'\n",
    "        data['node_id'] = data['trackId'].astype(str)\n",
    "\n",
    "        # apply data scale as same as PECnet\n",
    "        data['x'] = data['x']/50\n",
    "        data['y'] = data['y']/50\n",
    "\n",
    "        # Mean Position\n",
    "        data['x'] = data['x'] - data['x'].mean()\n",
    "        data['y'] = data['y'] - data['y'].mean()\n",
    "\n",
    "        max_timesteps = data['frame'].max()\n",
    "\n",
    "        if len(data) > 0:\n",
    "            scene = Scene(timesteps=max_timesteps+1, dt=dt, name=\"sdd_\" + data_class, aug_func=augment if data_class == 'train' else None)\n",
    "            n=0\n",
    "            # print(pd.unique(data['node_id']))\n",
    "            for node_id in pd.unique(data['node_id']):\n",
    "                nodes_df = data[data['node_id'] == node_id]\n",
    "                for meta_id in pd.unique(nodes_df['metaId']):\n",
    "                    node_df = nodes_df[nodes_df['metaId'] == meta_id]\n",
    "                    # need to find a way to separate nodes with same id but not contiguous track\n",
    "                    if len(node_df) > 1:\n",
    "                        # if not np.all(np.diff(node_df['frame'])==1):\n",
    "                        #     if data_class == \"train\":\n",
    "                        #         node_df.to_csv('AAA/train_'+node_id+'.csv', sep='\\t')\n",
    "                        #     else:\n",
    "                        #         node_df.to_csv('AAA/test_'+node_id+'.csv', sep='\\t')\n",
    "\n",
    "                        \n",
    "                        assert np.all(np.diff(node_df['frame']) == 1)                         \n",
    "                        node_values = node_df[['x', 'y']].values\n",
    "\n",
    "                        if node_values.shape[0] < 2:\n",
    "                            continue\n",
    "\n",
    "                        new_first_idx = node_df['frame'].iloc[0]\n",
    "\n",
    "                        x = node_values[:, 0]\n",
    "                        y = node_values[:, 1]\n",
    "                        vx = derivative_of(x, scene.dt)\n",
    "                        vy = derivative_of(y, scene.dt)\n",
    "                        ax = derivative_of(vx, scene.dt)\n",
    "                        ay = derivative_of(vy, scene.dt)\n",
    "\n",
    "                        data_dict = {('position', 'x'): x,\n",
    "                                    ('position', 'y'): y,\n",
    "                                    ('velocity', 'x'): vx,\n",
    "                                    ('velocity', 'y'): vy,\n",
    "                                    ('acceleration', 'x'): ax,\n",
    "                                    ('acceleration', 'y'): ay}\n",
    "\n",
    "                        node_data = pd.DataFrame(data_dict, columns=data_columns)\n",
    "                        node = Node(node_type=env.NodeType.PEDESTRIAN, node_id=node_id, data=node_data)\n",
    "                        node.first_timestep = new_first_idx\n",
    "\n",
    "                        scene.nodes.append(node)\n",
    "            if data_class == 'train':\n",
    "                scene.augmented = list()\n",
    "                angles = np.arange(0, 360, 15) if data_class == 'train' else [0]\n",
    "                for angle in angles:\n",
    "                    scene.augmented.append(augment_scene(scene, angle))\n",
    "\n",
    "            print(scene)\n",
    "            scenes.append(scene)\n",
    "    env.scenes = scenes\n",
    "\n",
    "    if len(scenes) > 0:\n",
    "        with open(data_out_path, 'wb') as f:\n",
    "            #pdb.set_trace()\n",
    "            dill.dump(env, f, protocol=dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SDD train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>trackId</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>sceneId</th>\n",
       "      <th>metaId</th>\n",
       "      <th>node_type</th>\n",
       "      <th>node_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.205903</td>\n",
       "      <td>2.980309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2.365903</td>\n",
       "      <td>2.980309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.525903</td>\n",
       "      <td>2.980309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>2.675903</td>\n",
       "      <td>2.980309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>2.835903</td>\n",
       "      <td>3.000309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>2.995903</td>\n",
       "      <td>3.000309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>3.155903</td>\n",
       "      <td>3.000309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>3.275903</td>\n",
       "      <td>3.000309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>3.435903</td>\n",
       "      <td>3.000309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>3.515903</td>\n",
       "      <td>3.050309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>3.515903</td>\n",
       "      <td>3.050309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>3.545903</td>\n",
       "      <td>3.050309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>3.545903</td>\n",
       "      <td>3.050309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "      <td>3.585903</td>\n",
       "      <td>3.050309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>3.625903</td>\n",
       "      <td>3.050309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>3.625903</td>\n",
       "      <td>3.050309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>3.665903</td>\n",
       "      <td>3.050309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>3.665903</td>\n",
       "      <td>2.960309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>3.665903</td>\n",
       "      <td>2.920309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>3.665903</td>\n",
       "      <td>2.820309</td>\n",
       "      <td>bookstore_0</td>\n",
       "      <td>0</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    frame  trackId         x         y      sceneId  metaId   node_type  \\\n",
       "0       0      100  2.205903  2.980309  bookstore_0       0  PEDESTRIAN   \n",
       "1       1      100  2.365903  2.980309  bookstore_0       0  PEDESTRIAN   \n",
       "2       2      100  2.525903  2.980309  bookstore_0       0  PEDESTRIAN   \n",
       "3       3      100  2.675903  2.980309  bookstore_0       0  PEDESTRIAN   \n",
       "4       4      100  2.835903  3.000309  bookstore_0       0  PEDESTRIAN   \n",
       "5       5      100  2.995903  3.000309  bookstore_0       0  PEDESTRIAN   \n",
       "6       6      100  3.155903  3.000309  bookstore_0       0  PEDESTRIAN   \n",
       "7       7      100  3.275903  3.000309  bookstore_0       0  PEDESTRIAN   \n",
       "8       8      100  3.435903  3.000309  bookstore_0       0  PEDESTRIAN   \n",
       "9       9      100  3.515903  3.050309  bookstore_0       0  PEDESTRIAN   \n",
       "10     10      100  3.515903  3.050309  bookstore_0       0  PEDESTRIAN   \n",
       "11     11      100  3.545903  3.050309  bookstore_0       0  PEDESTRIAN   \n",
       "12     12      100  3.545903  3.050309  bookstore_0       0  PEDESTRIAN   \n",
       "13     13      100  3.585903  3.050309  bookstore_0       0  PEDESTRIAN   \n",
       "14     14      100  3.625903  3.050309  bookstore_0       0  PEDESTRIAN   \n",
       "15     15      100  3.625903  3.050309  bookstore_0       0  PEDESTRIAN   \n",
       "16     16      100  3.665903  3.050309  bookstore_0       0  PEDESTRIAN   \n",
       "17     17      100  3.665903  2.960309  bookstore_0       0  PEDESTRIAN   \n",
       "18     18      100  3.665903  2.920309  bookstore_0       0  PEDESTRIAN   \n",
       "19     19      100  3.665903  2.820309  bookstore_0       0  PEDESTRIAN   \n",
       "\n",
       "   node_id  \n",
       "0      100  \n",
       "1      100  \n",
       "2      100  \n",
       "3      100  \n",
       "4      100  \n",
       "5      100  \n",
       "6      100  \n",
       "7      100  \n",
       "8      100  \n",
       "9      100  \n",
       "10     100  \n",
       "11     100  \n",
       "12     100  \n",
       "13     100  \n",
       "14     100  \n",
       "15     100  \n",
       "16     100  \n",
       "17     100  \n",
       "18     100  \n",
       "19     100  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data_class in [\"train\", \"test\"]:\n",
    "    raw_path = \"raw_data/stanford\"\n",
    "    out_path = \"processed_data_noise\"\n",
    "    data_path = os.path.join(raw_path, f\"{data_class}_trajnet.pkl\")\n",
    "    print(f\"Processing SDD {data_class}\")\n",
    "    data_out_path = os.path.join(out_path, f\"sdd_{data_class}.pkl\")\n",
    "    df = pickle.load(open(data_path, \"rb\"))\n",
    "    env = Environment(node_type_list=['PEDESTRIAN'], standardization=standardization)\n",
    "    attention_radius = dict()\n",
    "    attention_radius[(env.NodeType.PEDESTRIAN, env.NodeType.PEDESTRIAN)] = 3.0\n",
    "    env.attention_radius = attention_radius\n",
    "\n",
    "    scenes = []\n",
    "\n",
    "    group = df.groupby(\"sceneId\")\n",
    "\n",
    "    for scene, data in group:\n",
    "        data['frame'] = pd.to_numeric(data['frame'], downcast='integer')\n",
    "        data['trackId'] = pd.to_numeric(data['trackId'], downcast='integer')\n",
    "\n",
    "        data['frame'] = data['frame'] // 12\n",
    "\n",
    "        data['frame'] -= data['frame'].min()\n",
    "\n",
    "        data['node_type'] = 'PEDESTRIAN'\n",
    "        data['node_id'] = data['trackId'].astype(str)\n",
    "\n",
    "        # apply data scale as same as PECnet\n",
    "        data['x'] = data['x']/50\n",
    "        data['y'] = data['y']/50\n",
    "\n",
    "        # Mean Position\n",
    "        data['x'] = data['x'] - data['x'].mean()\n",
    "        data['y'] = data['y'] - data['y'].mean()\n",
    "\n",
    "        max_timesteps = data['frame'].max()\n",
    "\n",
    "        if len(data) > 0:\n",
    "\n",
    "            scene = Scene(timesteps=max_timesteps+1, dt=dt, name=\"sdd_\" + data_class, aug_func=augment if data_class == 'train' else None)\n",
    "            n=0\n",
    "            for node_id in pd.unique(data['node_id']):\n",
    "\n",
    "                node_df = data[data['node_id'] == node_id]\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "node_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SDD train\n",
      "Processing SDD test\n"
     ]
    }
   ],
   "source": [
    "for data_class in [\"train\", \"test\"]:\n",
    "    raw_path = \"raw_data/stanford/longterm\"\n",
    "    out_path = \"processed_data_noise\"\n",
    "    data_path = os.path.join(raw_path, f\"{data_class}_longterm.pkl\")\n",
    "    print(f\"Processing SDD {data_class}\")\n",
    "    data_out_path = os.path.join(out_path, f\"sdd_{data_class}.pkl\")\n",
    "    df = pickle.load(open(data_path, \"rb\"))\n",
    "    df = swap_columns(df, 'trackId', 'frame')\n",
    "    env = Environment(node_type_list=['PEDESTRIAN'], standardization=standardization)\n",
    "    attention_radius = dict()\n",
    "    attention_radius[(env.NodeType.PEDESTRIAN, env.NodeType.PEDESTRIAN)] = 3.0\n",
    "    env.attention_radius = attention_radius\n",
    "\n",
    "    scenes = []\n",
    "\n",
    "    group = df.groupby(\"sceneId\")\n",
    "\n",
    "    for scene, data in group:\n",
    "        data['frame'] = pd.to_numeric(data['frame'], downcast='integer')\n",
    "        data['trackId'] = pd.to_numeric(data['trackId'], downcast='integer')\n",
    "\n",
    "        data['frame'] = data['frame'] // 30\n",
    "\n",
    "        data['frame'] -= data['frame'].min()\n",
    "\n",
    "        data['node_type'] = 'PEDESTRIAN'\n",
    "        data['node_id'] = data['trackId'].astype(str)\n",
    "\n",
    "        # apply data scale as same as PECnet\n",
    "        data['x'] = data['x']/50\n",
    "        data['y'] = data['y']/50\n",
    "\n",
    "        # Mean Position\n",
    "        data['x'] = data['x'] - data['x'].mean()\n",
    "        data['y'] = data['y'] - data['y'].mean()\n",
    "\n",
    "        max_timesteps = data['frame'].max()\n",
    "\n",
    "        if len(data) > 0:\n",
    "            scene = Scene(timesteps=max_timesteps+1, dt=dt, name=\"sdd_\" + data_class, aug_func=augment if data_class == 'train' else None)\n",
    "            n=0\n",
    "            # print(pd.unique(data['node_id']))\n",
    "            for node_id in pd.unique(data['node_id']):\n",
    "                nodes_df = data[data['node_id'] == node_id]\n",
    "                for meta_id in pd.unique(nodes_df['metaId']):\n",
    "                    node_df = nodes_df[nodes_df['metaId'] == meta_id]\n",
    "                    if len(node_df) > 1:\n",
    "                        assert np.all(np.diff(node_df['frame']) == 1)                         \n",
    "                        node_values = node_df[['x', 'y']].values\n",
    "\n",
    "                        if node_values.shape[0] < 2:\n",
    "                            continue\n",
    "\n",
    "                        new_first_idx = node_df['frame'].iloc[0]\n",
    "\n",
    "                        x = node_values[:, 0]\n",
    "                        y = node_values[:, 1]\n",
    "                        vx = derivative_of(x, scene.dt)\n",
    "                        vy = derivative_of(y, scene.dt)\n",
    "                        ax = derivative_of(vx, scene.dt)\n",
    "                        ay = derivative_of(vy, scene.dt)\n",
    "\n",
    "                        data_dict = {('position', 'x'): x,\n",
    "                                    ('position', 'y'): y,\n",
    "                                    ('velocity', 'x'): vx,\n",
    "                                    ('velocity', 'y'): vy,\n",
    "                                    ('acceleration', 'x'): ax,\n",
    "                                    ('acceleration', 'y'): ay}\n",
    "\n",
    "                        node_data = pd.DataFrame(data_dict, columns=data_columns)\n",
    "                        node_data.to_csv('AAA/longterm/'+data_class+'nodeid'+str(node_id)+'_metaid'+str(meta_id)+'.csv', sep='\\t')\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SDD train\n",
      "Processing SDD test\n"
     ]
    }
   ],
   "source": [
    "for data_class in [\"train\", \"test\"]:\n",
    "    raw_path = \"raw_data/stanford\"\n",
    "    out_path = \"processed_data_noise\"\n",
    "    data_path = os.path.join(raw_path, f\"{data_class}_trajnet.pkl\")\n",
    "    print(f\"Processing SDD {data_class}\")\n",
    "    data_out_path = os.path.join(out_path, f\"sdd_{data_class}.pkl\")\n",
    "    df = pickle.load(open(data_path, \"rb\"))\n",
    "    env = Environment(node_type_list=['PEDESTRIAN'], standardization=standardization)\n",
    "    attention_radius = dict()\n",
    "    attention_radius[(env.NodeType.PEDESTRIAN, env.NodeType.PEDESTRIAN)] = 3.0\n",
    "    env.attention_radius = attention_radius\n",
    "\n",
    "    scenes = []\n",
    "\n",
    "    group = df.groupby(\"sceneId\")\n",
    "\n",
    "    for scene, data in group:\n",
    "        data['frame'] = pd.to_numeric(data['frame'], downcast='integer')\n",
    "        data['trackId'] = pd.to_numeric(data['trackId'], downcast='integer')\n",
    "\n",
    "        data['frame'] = data['frame'] // 12\n",
    "\n",
    "        data['frame'] -= data['frame'].min()\n",
    "\n",
    "        data['node_type'] = 'PEDESTRIAN'\n",
    "        data['node_id'] = data['trackId'].astype(str)\n",
    "\n",
    "        # apply data scale as same as PECnet\n",
    "        data['x'] = data['x']/50\n",
    "        data['y'] = data['y']/50\n",
    "\n",
    "        # Mean Position\n",
    "        data['x'] = data['x'] - data['x'].mean()\n",
    "        data['y'] = data['y'] - data['y'].mean()\n",
    "\n",
    "        max_timesteps = data['frame'].max()\n",
    "\n",
    "        if len(data) > 0:\n",
    "\n",
    "            scene = Scene(timesteps=max_timesteps+1, dt=dt, name=\"sdd_\" + data_class, aug_func=augment if data_class == 'train' else None)\n",
    "            n=0\n",
    "            for node_id in pd.unique(data['node_id']):\n",
    "\n",
    "                node_df = data[data['node_id'] == node_id]\n",
    "                if len(node_df) > 1:\n",
    "                    assert np.all(np.diff(node_df['frame']) == 1)                         \n",
    "                    node_values = node_df[['x', 'y']].values\n",
    "\n",
    "                    if node_values.shape[0] < 2:\n",
    "                        continue\n",
    "\n",
    "                    new_first_idx = node_df['frame'].iloc[0]\n",
    "\n",
    "                    x = node_values[:, 0]\n",
    "                    y = node_values[:, 1]\n",
    "                    vx = derivative_of(x, scene.dt)\n",
    "                    vy = derivative_of(y, scene.dt)\n",
    "                    ax = derivative_of(vx, scene.dt)\n",
    "                    ay = derivative_of(vy, scene.dt)\n",
    "\n",
    "                    data_dict = {('position', 'x'): x,\n",
    "                                ('position', 'y'): y,\n",
    "                                ('velocity', 'x'): vx,\n",
    "                                ('velocity', 'y'): vy,\n",
    "                                ('acceleration', 'x'): ax,\n",
    "                                ('acceleration', 'y'): ay}\n",
    "\n",
    "                    node_data = pd.DataFrame(data_dict, columns=data_columns)\n",
    "                    node_data.to_csv('AAA/shortterm/'+data_class+'nodeid'+str(node_id)+'.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
